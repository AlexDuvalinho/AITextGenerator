# AI TEXT GENERATOR

## Context

This project has been realized as a part of Natural Langage Processing class teached by Matthias Gallé, *Naverlabs* for the MSc AI  (CentraleSupelec 2019/2020). Project members are: 

- Gaël de Léséleuc
- Alexandre Duval
- Thomas Lamson

## Project description

A detailed description of the project can be found in the *report* folder. Roughly, the idea is to develop an automatic advanced writing tool to help authors in their task. Concretely, the final intended product is a web-service where authors can write text and ask for paragraph automatic generation based on the previous text, the following text, desired size/theme and an optional summary of the content to generate.

## Source code structure

- **json_generation** module handles all the text preprocessing : from raw text to json file containing the novel split by paragraph and for each of them several informations : the list of entities inside the paragraph, size, summaries, etc 
- **torch_loader** module is used to load and vectorize on the fly the preprocess json file so that in can be directly feed into a GPT2 model for fine-tuning
- **model_training** contains the script to fine-tune the GPT2 model, it is simply a small adaptation of huggingface/run_langage_modeling to allow the use of our custom dataset
- **model_evaluation** module will be used to evaluate the output quality of a fine-tune GPT2 model 
- **model_use** module interface our GPT2 fine-tune model with the web_service backend 
- **web_server** contains the back_end interface of our web service. The web service front-end has been pushed to a separate repo and can been found at : https://github.com/WeazelDev/AITextGeneratorFront
- **third_party** folder contains several framework that has been cloned directly into our project 

## Download the archives

There are two archives you can download for this project: the data archive and the models archive.

### Data archive

It contains all the input and output data extracted and generated by this project (~180 Mo).

1. Download it at: https://drive.google.com/open?id=19b_x5dsie21Z6ZW7R6vnvwN3IPaKPXMv
2. Extract it direclty in the project root.

### Models archive

It contains the weights of the different models used and/or trained during this project (~1.2 Go).

1. Download it at: https://drive.google.com/open?id=1svTqyugLI36zaX6Fo6hr-Od4ATRxH2vf
2. Extract it directly in the project root.

## Script 

Once the project is installed in a clean environment : 

```
git clone https://github.com/WeazelDev/AITextGenerator.git
cd AITextGenerator
pip install -r requirements.txt
```

In order to make our experiments, we used the following script that can be found on project root 
- **splitter.py**: to split the raw text file in paragraph and save them in json files 
- **ner.py**: to perform entities recognition on the paragrap 
- **summarization.py**: to summarize with differents summarizer the paragraph 
- **finetuning.py** : to finetune GPT. We used the [fine-tuning script proposed by Huggingface](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py). We make a few changes to be able to load our custom torch dataset and correctly handle some specifity of our projects. 
-- **evaluation.py** : to generate text with a fine-tune GPT2 and compute our custom metrics on the generated paragraph 
