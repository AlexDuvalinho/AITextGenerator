# AI TEXT GENERATOR

## Context

This project has been realized as a part of Natural Langage Processing class teached by Matthias Gallé, *Naverlabs* for the MSc AI  (CentraleSupelec 2019/2020). Project members are: 

- Gaël de Léséleuc
- Alexandre Duval
- Thomas Lamson

## Project description

A detailed description of the project can be found in the *report* folder. Roughly, the idea is to develop an automatic writing tool to help authors in their task. The final intended product is a web-service where authors can write text and ask for paragraph automatic generation based on previous text, following text, desired size/theme, a list of entities to mention in the generation as well as an optional summary of the content. More concretly, we fine-tune an OpenAI GPT2 model on novel specific data for controllable and contextualised text generation.

## Source code structure

- **json_generation** handles all the text preprocessing : from raw text and metadata (extracted from Gutenberg) to final json file containing the novel split by paragraph and related information : the list of entities inside the paragraph, size, summaries, etc 
- **torch_loader** module is used to load and vectorize on the fly the data (preprocessed json file) so that it can be directly feed into a GPT2 model for fine-tuning
- **model_training** contains the script to fine-tune the GPT2 model. It is simply an adaptation of huggingface/run_langage_modeling that allows its use on our custom dataset
- **model_evaluation** module used to evaluate the output quality of our fine-tuned GPT2 model 
- **model_use** module interface our GPT2 fine-tune model with the web_service backend 
- **web_server** contains the back_end interface of our web service. The web service front-end has been pushed to a separate repo and can been found at : https://github.com/WeazelDev/AITextGeneratorFront
- **third_party** folder contains several framework that has been cloned directly into our project. 

## Download the archives

There are two archives you can download for this project: the data archive and the models archive.

### Data archive

It contains all the input and output data extracted and generated by this project (~180 Mo).

1. Download it at: https://drive.google.com/open?id=19b_x5dsie21Z6ZW7R6vnvwN3IPaKPXMv
2. Extract it direclty in the project root.

Refer to the data.py file to observe the framework we utilised to obtain this data. If you want to process new books or to re-run the code, you first need to populate the Gutenberg cache first (long process). To do so, uncomment the first lines of the code in main.py. 

### Models archive

It contains the weights of the different models used and/or trained during this project (~1.2 Go).

1. Download it at: https://drive.google.com/open?id=1svTqyugLI36zaX6Fo6hr-Od4ATRxH2vf
2. Extract it directly in the project root.

## Script 

Once the project is installed in a clean environment : 

```
git clone https://github.com/WeazelDev/AITextGenerator.git
cd AITextGenerator
pip install -r requirements.txt
```

In order to make our experiments, we used the following script that can be found on project root 
- **splitter.py**: to split the raw text file in paragraph and save them in json files 
- **ner.py**: to perform entities recognition on the paragrap 
- **summarization.py**: to summarize with differents summarizer the paragraph 
- **finetuning.py** : to finetune GPT. We used the [fine-tuning script proposed by Huggingface](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py). We make a few changes to be able to load our custom torch dataset and correctly handle some specifity of our projects. 
-- **evaluation.py** : to generate text with a fine-tune GPT2 and compute our custom metrics on the generated paragraph 
